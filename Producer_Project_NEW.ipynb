{"cells":[{"cell_type":"code","source":["##############################################################################\n\n###### Producer of Financial Data Stream morphed into Meteo Data Stream ######\n\n##############################################################################\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68636a1a-5173-4ef3-b77e-baf115a56ffd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import socket\nimport time\nimport pandas as pd\nfrom datetime import datetime\nfrom pyspark import SparkFiles\n\n### Upload the csv-files from the external website ###\n### Import 12 files for 12 months 2022\n\nurl1 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202201.csv\"\nurl2 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202202.csv\"\nurl3 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202203.csv\"\nurl4 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202204.csv\"\nurl5 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202205.csv\"\nurl6 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202206.csv\"\nurl7 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202207.csv\"\nurl8 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202208.csv\"\nurl9 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202209.csv\"\nurl10 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202210.csv\"\nurl11 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202211.csv\"\nurl12 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202212.csv\"\n\nspark.sparkContext.addFile(url1)\ninputFile1 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202201.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url2)\ninputFile2 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202202.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url3)\ninputFile3 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202203.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url4)\ninputFile4 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202204.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url5)\ninputFile5 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202205.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url6)\ninputFile6 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202206.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url7)\ninputFile7 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202207.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url8)\ninputFile8 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202208.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url9)\ninputFile9 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202209.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url10)\ninputFile10 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202210.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url11)\ninputFile11 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202211.csv\"), header=True, inferSchema=True, sep=';')\n\nspark.sparkContext.addFile(url12)\ninputFile12 = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202212.csv\"), header=True, inferSchema=True, sep=';')\n \n### Combine the 12 singel dataframes into one single one\n\nMega_inputFile = pd.concat([inputFile1, inputFile2, inputFile3, inputFile4, inputFile5, inputFile6, inputFile7, inputFile8, inputFile9, inputFile10, inputFile11, inputFile12], ignore_index = True, sort = False)\n\n### This function is needed for the transformation of the files into Pandas data frames\n\nspark.conf.set(\"spark.sql.execcution.arrow.enabled\",\"true\")\n\n### Start the sending out of the extended dataFrame\n\ndef send():\n  \n  ifile = Mega_inputFile.toPandas()\n  i = 0\n\n  while True:\n    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    client_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    client_socket.bind(('localhost', 9998))\n    client_socket.listen(10)\n    conn, addr = client_socket.accept()\n    try:  \n      while True:\n        start = time.time()\n        for row in ifile.iterrows():\n          print(\"sending: \"+ row.rstrip() + \",\" + str(datetime.now()))\n          message = row.rstrip() + ',' + str(datetime.now()) + \"\\n\"\n          message = message.encode()\n          conn.send(message)\n          # send data every 100 ms\n          #TODO: change the input rate here (or remove) to see effects in the consumer on total number of items processed per window\n          time.sleep(0.1)\n\n    except Exception as e:\n        print(str(e))\n        conn.close()\n        client_socket.close()\n        continue\n    finally:\n        conn.close()\n        client_socket.close()\nsend()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69b4b1cc-1d38-489e-b2f5-5eec327b2be6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a310458-de95-4433-9450-95c6ec5d2870","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"name":"netcat","notebookId":1851672597797230,"application/vnd.databricks.v1+notebook":{"notebookName":"Producer_Project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4323768402495868,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":4323768402495865}},"nbformat":4,"nbformat_minor":0}
