{"cells":[{"cell_type":"code","source":["\n# Orginal Workbook: Price Anomaly Detection for Stock Exchange Data using Structured Streaming \n\n######################################## Morphing into the stream of meteo data ##############################################\n\nThe goal of this exercise is to analyse real stock exchange data using Spark Structured Streaming and to detect anomalies in the price fluctuation. In order to achieve this, the Structured Streaming consumer (running in this notebook) will read financial data from a socket stream (make sure to start the Producer first in order to have this data available), compute a few aggregations on the stream and compare the new results with the historical values.\n\nAs a first step, we must parse the strings read from the socket stream, in order to be able to further on process the incoming data in a structured format. This parsing is already provided to you in the code block below. Have a look through the structure of the CSV input file used in the Producer to make sure you understand how the code below works."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70509abc-8f50-4ab0-b2ae-7673b8800f42","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nimport socket, time\n\n# Simon's project  pull function for selecting the rows\n\nfrom pyspark.sql.functions import col\n\n# Too high share price increase in stock\n#=======================================\n\n# This code seeks through the NYSE stock exchange data to see which\n# transactions are unusually priced, as compared to the overall previous price average.\n\n# Look at the CSV input file to understand how the data is structured\n\n# exchange company date price_open price_high price_low price_close stock_volume price_adj_close timestamp\n# NYSE\tASP\t2001-12-31\t12.55\t12.8\t12.42\t12.8\t11300\t6.91 2018-02-14 13:16:44.550444\n   \n  \n# 1. Read input stream from socket (by default, sockets contain raw strings, which we must then parse in a structured format)\n\nlines = spark.readStream.format(\"socket\")\\\n  .option(\"host\", \"localhost\")\\\n  .option(\"port\", 9998)\\\n  .load()\n\nstructuredStream = lines.select(\\\n  split(lines.value, \",\")[0].alias(\"starttime\"),\\\n  split(lines.value, \",\")[1].alias(\"site\"),\\\n  split(lines.value, \",\")[2].alias(\"masl\"),\\\n  split(lines.value, \",\")[3].alias(\"magl\"),\\\n  split(lines.value, \",\")[4].alias(\"x\"),\\\n  split(lines.value, \",\")[5].alias(\"y\"),\\\n  split(lines.value, \",\")[6].alias(\"sensor\"),\\\n  split(lines.value, \",\")[7].cast('Float').alias(\"temperature\"),\\\n  split(lines.value, \",\")[8].cast('Float').alias(\"humidity\"))  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8eee654d-fa23-496c-a0e7-056a7a4d6f0d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Generate the running query\n\nAfter reading in the raw stream, we need to first apply a few transformations in order to only keep the relevant data for the anomaly detection.\n\n## 1. Window computation\n\nFirst, we want to group and process streaming data in windows of time. For this exercise, we will apply transformations on each **4 seconds** of data. For this purpose, we need to group by a window of 4 seconds, based on the field that identifies the time of each row in the structured stream.\n\nFill in the code below in order to achieve this (you can see an example of how to achieve this in the lecture slides)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc8358db-eb1e-4d2f-a9da-715a4c70a0a5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Simon's project: move the slicing and dicing here as: 'GroupedData' object has no attribute 'select'\n\nSliceDice01 = structuredStream.select(\"starttime\",\"site\",\"temperature\")\n\nSliceDice02 = SliceDice01.where(col(\"site\") == \"Winterthur Bahnhof\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39553d97-202b-4a6d-8214-3e7f43d1bc8c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Generate window computation\n\n# Simon's project streaming window ond sliced ond diced data\n\nwindowedStream = SliceDice02.groupBy(window(\"starttime\",\"24 hours\",\"24 hours\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"826287de-d5e9-46e6-8070-6a834f5ab063","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## 2. Aggregations\n\n## Once we have generated a windowed computation on the raw stream, we need to also define the aggregations that we want to perform on the data. It is always good practice to use built-in aggregations of Spark whenever        possible (for example, using the *agg* function) and to only keep the relevant data from the structured stream, as oposed to keeping in memory the entire stream, which might lead to memory issues. \n\n## Discarding unrelevant data can be achieved in 2 ways:\n\n\n## 1. always performing aggregations instead of keeping the raw data in memory (e.g. for a *price* column, only keep the average/max/min value per window instead of all the data points)\n\n\n## Revelant for Simon's project:\n\n## 2. in cases where this is not possible, always explicitly select only the fields of interest and drop the remaining ones\n\n## These two measures ensure that you will not run into memory problems, even if the streaming dataframe grows continuously."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ea0c26b-b2c2-4637-8db7-7d1fb0e5b707","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In this exercise we will use the approach 1. - namely, storing only aggregations instead of raw data. \n\nFill in the transformations below, using the *avg*, *count* and *sum* aggregations. Keep in mind that the goal of the exercise is to:\n\n  a. compute the average price per window - this can be done straightforwardly by using a Spark built-in function;\n\n  b. compare the window average with the overall (historic) average (over all windows) - this will require also counting the number of elements in each window and their total price.\n  \nFinally, to make it easier to use the aggregation columns later on, make sure to also assign them intuitive names, by using the *alias* function."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7699a0a2-7587-44bd-a7c7-19f2b9b48d24","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#### Reactivated aggregation and adjusted to catch average temperature per day:\n\nagregationsStream = windowedStream.agg(avg(\"temperature\").alias(\"window_temp_average\"), count(\"starttime\").alias(\"count_temps\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1b3a573-a654-4ec6-a4cb-4657c4425ceb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Starting the stream\n\nNote that so far, all we have done is to create a logical plan for Spark to handle the structured stream. In order to start the actual computation, we need to use the *writeStream* function and then explicitly *start()* the corresponding stream. See an example in the lectures on how to achieve this. Additionally, make sure to assign a name for the resulting query handle (using the *queryName()* function), in order to be able to directly reference it as a table using the SQL context later on."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c94a9f59-8e33-452b-beb4-d1d95b4cfba6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# NOTE: always use 'complete' outputMode for aggregations and 'append' outputMode to get complete records (the entire stream)\n\n# Simon's project: last modification is the windowedStream, the slicing and dicing that replace the aggregations were performed prior to the windowedStream\n\n# The original algorithm with aggregation as last data modification\n\nstreamingETLQuery =\\\nagregationsStream \\\n  .writeStream \\\n  .format(\"memory\") \\\n  .queryName(\"aggDF\") \\\n  .outputMode(\"complete\")\\\n  .start()  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ec040e7-ac3f-439c-b9b5-7ce08ff602b7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Visualizing the stream contents\n\nYou can display a structured stream dataframe in the same way you would a regular dataframe. Databricks will automatically refresh the table displayed.\n\nTo try this out, first cancel the previous command (which started writing the stream) and then run the display command below on the stream variable. Once the stream starts, you will see an ID replacing the \"Stream initializing\" message (it might take a while). Click on the green icon that will appear underneath the command, in order to display statistics about the stream as well as its contents. \n\nNotice that data will not necessarily be displayed in order of arrival. In order to see the most recent data first, also sort the stream by the window start time when displaying.\n\nNOTE: the *display* command will only work as presented above in the Databricks environment (not in a local installation)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d8e987a5-c571-454a-a514-d2c92f81516c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n#display(agregationsStream)\ndisplay(agregationsStream.sort(desc(\"window.start\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5f07ebc-0a37-4cfe-a741-50595a90f4fa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Computation and visualization of the resulting streaming dataframe\n\nFinally we are able to use the structured stream in order to detect anomalies in the stock exchange prices. Fill in the final TODOs below in order to achieve this. Make sure to start the Producer before running all the code blocks in this Notebook to get the results."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e3fb3cb-d9c4-4ea1-bd73-56594ca9f977","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import math\nimport sys\nfrom pyspark.sql.functions import desc\n\n#TODO: complete here the SQL statement required to get all the contents of the stream (use the name assigned earlier)\ndf = spark.sql(\"select * from aggDF\")\nprint(\"Running SQL (initialization may take a while)...\")\n\niter = 1\n\n# initialize the historic average with some random value\nprev_avg = 1.0\nold_count = 0\n\nwhile True:\n  \n  # only start computing once some data has been collected (note that the dataframe is automatically updated by Spark)\n  if(df.count() != 0):\n    while(old_count == df.count()):\n      # don't do anything while there is no new data\n      continue\n      \n    # update the total count in order to be able to use the condition above in the next iteration\n    old_count = df.count()\n    \n    print(\"*********************************************************************************************************\")\n    print(\"\\nIteration no. \"+ str(iter) + \"\\n\")\n    print(\"Sample data from streaming dataframe:\\n\")\n    \n    # set the time to be the end of the window (just to have a visual indication of time)\n    df = df.withColumn(\"time\", df.window.end)\n       \n    # we sort by time first, because by default there is no ordering in the streaming dataframe and we want to only show the most recent results\n    # the following allows showing the most recent 5 windows (the False parameter instructs Spark not to truncate the output)\n    df = df.sort(desc(\"time\"))\n    df.show(5, False)\n\n    print(\"Current number of windows processed in stream: \"+ str(old_count) + \"\\n\")\n\n    #TODO: use here the field that denotes the sum of prices per window.\n    # In order to get the overall historic sum, you must add all the prices per window (use a new aggregate to do this)\n    val = df.select('price_sum').agg(sum('price_sum')).collect()[0][0]\n\n    #TODO: the same for the counts of all items with prices\n    counts = df.select('count_prices').agg(sum('count_prices')).collect()[0][0]\n    if(counts != None and counts != 0):\n      # update the historic average taking into account the new aggregated values\n      historic_avg = val/counts\n     \n   \n    # the following lines just handle pretty-printing the output...\n    for indent in range(iter - 1):\n      sys.stdout.write(\"|****|\")\n    print (\"|----|  Average price before current window: \" + str(prev_avg))\n    sys.stdout.flush()\n    \n    #TODO: take the most recent window average - use the field name you assigned for the average price per window\n    # since we already ordered by time, you can simply take the first element of the result\n    current_avg = df.select(\"window_price_average\").take(1)[0][0]\n    \n    # the following lines just handle pretty-printing the output...\n    for indent in range(iter - 1):\n      sys.stdout.write(\"      \") \n    sys.stdout.write(\"|****| \" + \" Current window average price: \" + str(current_avg))\n    print\n\n    # compute the percentage change from the historic average up until this window (ignoring the first iteration, where the historic average is a random number)\n    if((math.fabs(current_avg - prev_avg)/prev_avg) > 0.3 and iter > 1):\n        print(\"\\n\\t!!! ANOMALY DETECTED: price fluctuated by \"+ str(float(\"{0:.2f}\".format(math.fabs(current_avg - prev_avg)/prev_avg)) * 100) + \" % !!!\\n\\n\\n\")\n        \n    prev_avg = historic_avg\n    iter += 1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8edc9a9-3c3f-4544-8132-b69d19b526e1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18cf5f46-903d-43e5-8b47-15e238793b06","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"name":"StructTest sliding stream","notebookId":3417154191564475,"application/vnd.databricks.v1+notebook":{"notebookName":"Structured Streaming_Project_Pre-Study","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":364985776790753}},"nbformat":4,"nbformat_minor":0}
