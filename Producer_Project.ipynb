{"cells":[{"cell_type":"markdown","source":["#Financial Data Stream Producer\n\nThis notebook provides some basic functionality which you need to let a stream run so that the Spark Streaming Engine can process it. The stream is produced from a file with trading data from the New York Stock Exchange for the years 2010-2011. In order to simulate streaming data, the producer reads this file continuously and adds a timestamp. \n\nThe only thing you need **to do** in order to run this Notebook is to change the path to the input file, after you upload it to Databricks. From here, the financial streaming data will be produced on a local socket on port 9998 (see Cmd 2 below).\n\nThe simplest way is to click `Run All` on the top. <br>\n\n**NOTE:** when replacing the inputFile below with your own after uploading the file to Databricks, don't forget to **copy the uploaded input file from \"dbfs:\" to \"file:\"** (as in the example below). This is because we are using native Python to open this file, therefore we need to provide the full path. Databricks does this conversion automatically whenever we open files using Spark API."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9de6fab-61f3-4ea9-99d0-181981e68290","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["###########################################################################\n\n###### Financial Data Stream Producer morphed into Meteo Data Stream #####\n\n###########################################################################\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68636a1a-5173-4ef3-b77e-baf115a56ffd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import socket\nimport time\nimport pandas as pd\nfrom datetime import datetime\nfrom pyspark import SparkFiles\n\n# Upload the csv-fles from the external website\n\nurl1 = \"https://www.web.statistik.zh.ch/awel/LoRa/data/AWEL_Sensors_LoRa_202208.csv\"\nspark.sparkContext.addFile(url1)\ninputFile = spark.read.csv(\"file://\"+SparkFiles.get(\"AWEL_Sensors_LoRa_202208.csv\"), header=True, inferSchema=True, sep=';')\n \n# This function is needed for the transformation into Pandas data frame\n\nspark.conf.set(\"spark.sql.execcution.arrow.enabled\",\"true\") \n\ndef send():\n  \n  ifile = inputFile.toPandas()\n  i = 0\n\n  while True:\n    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    client_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    client_socket.bind(('localhost', 9998))\n    client_socket.listen(10)\n    conn, addr = client_socket.accept()\n    try:  \n      while True:\n        start = time.time()\n        for row in ifile.iterrows():\n          print(\"sending: \"+ row.rstrip() + \",\" + str(datetime.now()))\n          message = row.rstrip() + ',' + str(datetime.now()) + \"\\n\"\n          message = message.encode()\n          conn.send(message)\n          # send data every 100 ms\n          #TODO: change the input rate here (or remove) to see effects in the consumer on total number of items processed per window\n          time.sleep(0.1)\n\n    except Exception as e:\n        print(str(e))\n        conn.close()\n        client_socket.close()\n        continue\n    finally:\n        conn.close()\n        client_socket.close()\nsend()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69b4b1cc-1d38-489e-b2f5-5eec327b2be6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"cb193144-362d137d23b6d2979482e459"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"'tuple' object has no attribute 'rstrip'\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["'tuple' object has no attribute 'rstrip'\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a310458-de95-4433-9450-95c6ec5d2870","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"name":"netcat","notebookId":1851672597797230,"application/vnd.databricks.v1+notebook":{"notebookName":"Producer_Project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4323768402495868,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":4323768402495865}},"nbformat":4,"nbformat_minor":0}
